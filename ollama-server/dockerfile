# AI_Avatar/ollama-server/Dockerfile

FROM ollama/ollama:latest

# Create startup script with fixed model pulling and proper binding
COPY <<EOF /usr/local/bin/start.sh
#!/bin/bash

# Set binding to all interfaces
export OLLAMA_HOST=0.0.0.0

# Start Ollama server in the background with proper binding
ollama serve &

# Wait for Ollama to be ready
until curl -s \$OLLAMA_HOST:11434 > /dev/null 2>&1; do
    echo "Waiting for Ollama server to be ready..."
    sleep 2
done

echo "Ollama server is ready. Pulling models..."

# Pull models one by one with proper quoting
echo "Pulling mxbai-embed-large:latest..."
if ! ollama pull mxbai-embed-large:latest; then
    echo "Failed to pull mxbai-embed-large:latest"
    exit 1
fi

echo "Pulling deepseek-r1:1.5b..."
if ! ollama pull deepseek-r1:1.5b; then
    echo "Failed to pull deepseek-r1:1.5b"
    exit 1
fi

echo "Here are available models:"
ollama list

# Keep container running - using the Ollama process directly
wait
EOF

RUN chmod +x /usr/local/bin/start.sh

# Only expose the Ollama API port
EXPOSE 11434

ENTRYPOINT ["/usr/local/bin/start.sh"]