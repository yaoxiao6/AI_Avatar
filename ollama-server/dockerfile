# AI_Avatar/ollama-server/Dockerfile

FROM ollama/ollama:latest

# Install curl for healthchecks and Python for the health check server
RUN apt-get update && apt-get install -y curl python3

# Create startup script with fixed model pulling and proper binding
COPY <<EOF /usr/local/bin/start.sh
#!/bin/bash

# Set binding to all interfaces
export OLLAMA_HOST=0.0.0.0

# Start Ollama server in the background with proper binding
ollama serve &

# Wait for Ollama to be ready
until curl -s \$OLLAMA_HOST:11434/api/tags > /dev/null 2>&1; do
    echo "Waiting for Ollama server to be ready..."
    sleep 2
done

echo "Ollama server is ready. Pulling models..."

# Pull models one by one with proper quoting
echo "Pulling mxbai-embed-large:latest..."
if ! ollama pull mxbai-embed-large:latest; then
    echo "Failed to pull mxbai-embed-large:latest"
    exit 1
fi

echo "Pulling deepseek-r1:1.5b..."
if ! ollama pull deepseek-r1:1.5b; then
    echo "Failed to pull deepseek-r1:1.5b"
    exit 1
fi

ollama list

# Health check endpoint for Cloud Run - use PORT env var provided by Cloud Run
python3 -m http.server \${PORT:-8080} --bind \$OLLAMA_HOST &

# Keep container running
wait
EOF

RUN chmod +x /usr/local/bin/start.sh

# Expose the Ollama API port and the health check port
EXPOSE 11434
EXPOSE 8080

ENTRYPOINT ["/usr/local/bin/start.sh"]