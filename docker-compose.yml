# AI_Avatar/docker-compose.yml

version: '3.8'

services:
  ollama-server:
    build:
      context: ./ollama-server
      dockerfile: dockerfile
    image: ai-avatar/ollama-server:latest
    ports:
      - "11434:11434"  # Default Ollama port
    networks:
      - ai-network
    volumes:
      - ollama-data:/root/.ollama  # Persist Ollama models
    
  flask-rag:
    build:
      context: ./flask-rag
      dockerfile: dockerfile
    image: ai-avatar/flask-rag:latest
    ports:
      - "8080:8080"
    networks:
      - ai-network
    environment:
      # - OLLAMA_HOST=ollama-server
      # - OLLAMA_PORT=11434
      - OLLAMA_BASE_URL=http://ollama-server:11434
    depends_on:
      - ollama-server
      
  node-backend:
    build:
      context: ./node-backend
      dockerfile: dockerfile
    image: ai-avatar/node-backend:latest
    ports:
      - "4000:4000" # Not true on GCP Cloud Run
    networks:
      - ai-network
    environment:
      - FLASK_RAG_BASE_URL=http://flask-rag:5001
    depends_on:
      - flask-rag

networks:
  ai-network:
    driver: bridge

volumes:
  ollama-data: